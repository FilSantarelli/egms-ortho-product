{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94cefdb4",
   "metadata": {},
   "source": [
    "# An EGMS data script\n",
    "\n",
    "A seamless way to reproduce EGMS L3 ORTHO products from L2a or L2b products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a88bf7",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe8cf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask import delayed\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from shapely.geometry import Polygon, box\n",
    "\n",
    "from egms_io import ortho_writer, read_csv_or_zip\n",
    "from format_dataframe import (\n",
    "    FieldSpec,\n",
    "    OutputNamingConvention,\n",
    "    compute_pid,\n",
    "    format_dataframe,\n",
    ")\n",
    "from metadata import generate_output_name\n",
    "from model_estimation import model_estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612f0135",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "Define the spatial/temporal sampling plan and AOI upfront so every later stage (gridding, interpolation, LS solving) can rely on consistent spacing, anchoring rules, acquisition window, and burst inventory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e11af32",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_bursts: list[Path] = list(Path.cwd().glob(\"test/input/*.zip\"))\n",
    "aoi: Polygon = box(4500000.0, 1700000.0, 4600000.0, 1800000.0)\n",
    "aoi_name: str = \"E45N17\"\n",
    "start_date: datetime = datetime.fromisoformat(\"2020-01-03\")\n",
    "end_date: datetime = datetime.fromisoformat(\"2024-12-25\")\n",
    "spacing: list[int] = [100, 100, 6]  # meters in easting, northing, days\n",
    "anchor: list[str] = [\"center\", \"center\", \"start\"]\n",
    "max_gap_days: int = 90  # maximum gap in days for interpolation\n",
    "output_dir: Path = Path.cwd() / \"output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e5a4ce",
   "metadata": {},
   "source": [
    "### Start distributed environment\n",
    "\n",
    "Spin up a lightweight Dask cluster; this can be changed to a different cluster, e.g. Slurm, PBS or Kubernetes, so that subsequent rasterization and least-squares steps can scale beyond a single node while still respecting queue policies; the snippet also prints the dashboard endpoint, which can be used to monitor the computation progress.\n",
    "\n",
    "In order to show the computation progress in JupyterLab, please create a new cluster through the Dask-JupyterLab web interface and copy-paste the scheduler address in the cell below; otherwise, a new cluster will be created, detached from the web interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1cda4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_address: str | None = None  # e.g. \"tcp://127.0.0.1:8786\"\n",
    "if scheduler_address is None:\n",
    "    cluster = LocalCluster(n_workers=20, memory_limit=0, processes=True)\n",
    "    client = Client(cluster)\n",
    "else:\n",
    "    client = Client(scheduler_address)\n",
    "print(f\"Dashboard available at: {client.dashboard_link}\")\n",
    "logger = logging.getLogger(\"egms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d467389",
   "metadata": {},
   "source": [
    "### Data ingestion\n",
    "\n",
    "Load each Sentinel-1 burst (CSV or zipped CSV) into memory via the shared `egms_io` helper, tagging rows with their source path so downstream diagnostics can trace anomalies back to the originating burst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b9e294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_ingestion(\n",
    "    path: Path,\n",
    "    *args: Any,\n",
    "    logger: logging.Logger | None = None,\n",
    "    **kwargs: Any,\n",
    " ) -> gpd.GeoDataFrame | tuple[gpd.GeoDataFrame, ...]:\n",
    "    \"\"\"Read a burst file (CSV or zipped CSV) and tag rows with their origin.\"\"\"\n",
    "    logger = logger or logging.getLogger(\"egms\")\n",
    "    out = read_csv_or_zip(path, *args, **kwargs)\n",
    "    record_count = len(out) if isinstance(out, gpd.GeoDataFrame) else len(out[0])\n",
    "    logger.info(\"Read %s with %d records\", path, record_count)\n",
    "    if isinstance(out, gpd.GeoDataFrame):\n",
    "        out[\"path\"] = path.stem\n",
    "    else:\n",
    "        out[0][\"path\"] = path.stem\n",
    "    logger.debug(\"Added 'path' column to data\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b220ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dfs = [\n",
    "    delayed(data_ingestion)(path, read_xml=False, logger=logger)\n",
    "    for path in input_bursts\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e760d983",
   "metadata": {},
   "source": [
    "### Time-interpolation procedure\n",
    "\n",
    "Normalize each burst’s timeline by interpolating missing acquisition dates to a regular cadence (`spacing[2]`). This yields dense, gap-free temporal stacks so cells share comparable observations when we solve the LS systems later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8c53f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_time_columns(\n",
    "    df: gpd.GeoDataFrame,\n",
    "    *,\n",
    "    start: datetime | None,\n",
    "    end: datetime | None,\n",
    "    spacing_days: int,\n",
    "    max_gap_days: int | None = None,\n",
    "    interpolate_kwargs: dict[str, Any] | None = None,\n",
    "    logger: logging.Logger | None = None,\n",
    " ) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Interpolate acquisition columns to a regular cadence in place.\"\"\"\n",
    "    log = logger or logging.getLogger(\"egms\")\n",
    "    log.info(\"Starting interpolation of time columns\")\n",
    "    frame = df.copy()\n",
    "    time_columns = sorted(\n",
    "        (col for col in frame.columns if re.fullmatch(r\"\\d+\", str(col))),\n",
    "        key=int,\n",
    "    )\n",
    "    log.debug(\"Identified time columns: %s\", time_columns)\n",
    "    if not time_columns:\n",
    "        return frame\n",
    "    numeric = frame[time_columns].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    time_index = pd.to_datetime(time_columns, format=\"%Y%m%d\")\n",
    "    start_ts = pd.Timestamp(start or time_index.min())\n",
    "    log.debug(\"Using start timestamp: %s\", start_ts)\n",
    "    end_ts = pd.Timestamp(end or time_index.max())\n",
    "    log.debug(\"Using end timestamp: %s\", end_ts)\n",
    "    freq = pd.Timedelta(days=int(spacing_days))\n",
    "    log.debug(\"Using frequency: %s\", freq)\n",
    "    target_index = pd.date_range(start=start_ts, end=end_ts, freq=freq)\n",
    "    log.debug(\"Generated target index with %d timestamps\", len(target_index))\n",
    "    if target_index.empty:\n",
    "        target_index = pd.DatetimeIndex([start_ts])\n",
    "    temporal_matrix = (\n",
    "        pd.DataFrame(\n",
    "            numeric.to_numpy().T,\n",
    "            index=time_index,\n",
    "            columns=frame.index,\n",
    "        ).reindex(target_index)\n",
    "    )\n",
    "    log.debug(\n",
    "        \"Reindexed temporal matrix to target index with shape %s\",\n",
    "        temporal_matrix.shape,\n",
    "    )\n",
    "    interp_opts: dict[str, Any] = {\n",
    "        \"axis\": 0,\n",
    "        \"limit_direction\": \"both\",\n",
    "        \"method\": \"time\",\n",
    "    }\n",
    "    if max_gap_days is not None:\n",
    "        interp_opts[\"limit\"] = math.ceil(max_gap_days / float(spacing_days))\n",
    "    if interpolate_kwargs:\n",
    "        interp_opts.update(interpolate_kwargs)\n",
    "    interpolated = temporal_matrix.interpolate(**interp_opts).ffill().bfill()\n",
    "    log.debug(\n",
    "        \"Interpolated temporal matrix with shape %s\",\n",
    "        interpolated.shape,\n",
    "    )\n",
    "    target_columns = [stamp.strftime(\"%Y%m%d\") for stamp in target_index]\n",
    "    interpolated = interpolated.T.set_axis(target_columns, axis=1)\n",
    "    frame = frame.drop(columns=time_columns, errors=\"ignore\").join(\n",
    "        interpolated,\n",
    "        how=\"left\",\n",
    "    )\n",
    "    log.info(\n",
    "        \"Completed interpolation of time columns with %d columns\",\n",
    "        len(target_columns),\n",
    "    )\n",
    "    return frame\n",
    "\n",
    "\n",
    "dfs = [\n",
    "    delayed(interpolate_time_columns)(\n",
    "        df,\n",
    "        start=start_date,\n",
    "        end=end_date,\n",
    "        spacing_days=spacing[2],\n",
    "        max_gap_days=max_gap_days,\n",
    "        logger=logger,\n",
    "    )\n",
    "    for df in raw_dfs\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c0541",
   "metadata": {},
   "source": [
    "### Rasterization\n",
    "\n",
    "Translate each observation into a spatio-temporal cell ID derived from the anchor-aware grid. The encoding packs x/y/time indices into a 64-bit integer so we can shuffle gigantic datasets through Dask while keeping the geometry reconstruction deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08835192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definizione grigliato di uscita\n",
    "spacing = np.array(spacing, dtype=int)\n",
    "for idx in range(3):\n",
    "    match anchor[idx]:\n",
    "        case \"start\":\n",
    "            anchor[idx] = 0\n",
    "        case \"center\":\n",
    "            anchor[idx] = int(spacing[idx] / 2)\n",
    "        case \"end\":\n",
    "            anchor[idx] = int(spacing[idx]) - 1\n",
    "        case _:\n",
    "            raise ValueError(\n",
    "                f\"Invalid {idx}-th anchor value: {anchor[idx]}\"\n",
    "            )\n",
    "anchor = np.array(anchor, dtype=int)\n",
    "\n",
    "xmin = aoi.bounds[0] - anchor[0]\n",
    "ymin = aoi.bounds[1] - anchor[1]\n",
    "tmin = start_date - timedelta(days=anchor[2].item())\n",
    "\n",
    "def date_to_days(date_str: str) -> int:\n",
    "    \"\"\"Convert an acquisition date (YYYYMMDD) into the grid time index.\"\"\"\n",
    "    date = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "    return int((date - tmin).days // spacing[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31e7ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cell_id(df: gpd.GeoDataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate flattened spatio-temporal records from per-burst columns.\n",
    "    \n",
    "    The ID is made up as follows:\n",
    "    - bits 0-15: cell index in easting\n",
    "    - bits 16-31: cell index in northing\n",
    "    - bits 32-63: cell index in time (days since tmin divided by spacing in days)\n",
    "    \"\"\"\n",
    "    cid_x = df.geometry.x.sub(xmin).floordiv(spacing[0]).astype(int)\n",
    "    cid_y = df.geometry.y.sub(ymin).floordiv(spacing[1]).astype(int)\n",
    "    granules: list[pd.DataFrame] = []\n",
    "    for col in df.columns:\n",
    "        if not re.fullmatch(r\"\\d+\", col):\n",
    "            continue\n",
    "        granule = pd.DataFrame(\n",
    "            {\n",
    "                \"los_east\": df[\"los_east\"],\n",
    "                \"los_up\": df[\"los_up\"],\n",
    "                \"los_north\": df[\"los_north\"],\n",
    "                \"height\": df[\"height_ortho\"],\n",
    "                \"gnss_velocity\": df[\"gnss_velocity\"],\n",
    "                \"displ\": df[col],\n",
    "                \"cid\": cid_x.values\n",
    "                + (cid_y.values << 16)\n",
    "                + (date_to_days(col) << 32),\n",
    "            }\n",
    "        )\n",
    "        granules.append(granule)\n",
    "    return pd.concat(granules, ignore_index=True, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d4fee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the final GeoDataFrame, recovering the cell spatio-temporal coordinates\n",
    "def cell_id_to_coords(cid: int) -> tuple[float, float, datetime]:\n",
    "    \"\"\"Decode a cell identifier back into x, y, time coordinates.\"\"\"\n",
    "    x_idx = cid & 0xFFFF\n",
    "    y_idx = (cid >> 16) & 0xFFFF\n",
    "    t_idx = (cid >> 32) & 0xFFFF\n",
    "    x = xmin + x_idx * spacing[0] + anchor[0]\n",
    "    y = ymin + y_idx * spacing[1] + anchor[1]\n",
    "    t = tmin + timedelta(days=int(t_idx * spacing[2] + anchor[2]))\n",
    "    return (x, y, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c35002",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [delayed(compute_cell_id)(df) for df in dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d433372",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.from_delayed(dfs, meta={\n",
    "    \"los_east\": \"float64\",\n",
    "    \"los_up\": \"float64\",\n",
    "    \"los_north\": \"float64\",\n",
    "    \"height\": \"float64\",\n",
    "    \"gnss_velocity\": \"float64\",\n",
    "    \"displ\": \"float64\",\n",
    "    \"cid\": \"int64\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcc6f77",
   "metadata": {},
   "source": [
    "Repartition the DataFrame to have partitions of ~4 MB, so that each partition can fit in memory during LS computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18299835",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(partition_size=\"4MB\", force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57963d0",
   "metadata": {},
   "source": [
    "### LS computation\n",
    "\n",
    "Group observations by cell ID and solve a per-cell 2×N least-squares problem (mixing ascending/descending looks) to recover east and up displacements. Cells lacking multi-geometry coverage are skipped to avoid unstable fits.\n",
    "\n",
    "A similar 3xN LS problem is also solved for retreiving GNSS information for each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3f7a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ls_solve(partition: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Solve east/up displacement and GNSS velocity for a cell partition.\"\"\"\n",
    "    asc_mask = partition.los_east < 0\n",
    "    # Exclusion conditions\n",
    "    excl_cond = asc_mask.all() # all ascending, no descending\n",
    "    excl_cond |= not asc_mask.any() # all descending, no ascending\n",
    "    excl_cond |= partition.size < 3 # less than three points\n",
    "    if excl_cond:\n",
    "        displ_sol = np.full(2, np.nan, dtype=float)\n",
    "        gnss_sol = np.full(3, np.nan, dtype=float)\n",
    "    else:\n",
    "        # Solve the problem for the displacement\n",
    "        design = partition[[\"los_east\", \"los_up\"]].to_numpy()\n",
    "        observed = partition[\"displ\"].to_numpy()\n",
    "        try:\n",
    "            displ_sol, *_ = np.linalg.lstsq(design, observed, rcond=None)\n",
    "        except np.linalg.LinAlgError:\n",
    "            displ_sol = np.full(2, np.nan, dtype=float)\n",
    "        # Solve the problem for the GNSS velocity\n",
    "        design = partition[[\"los_east\", \"los_up\", \"los_north\"]].to_numpy()\n",
    "        observed = partition[\"gnss_velocity\"].to_numpy()\n",
    "        try:\n",
    "            gnss_sol, *_ = np.linalg.lstsq(design, observed, rcond=None)\n",
    "        except np.linalg.LinAlgError:\n",
    "            gnss_sol = np.full(3, np.nan, dtype=float)\n",
    "\n",
    "    return pd.Series({\n",
    "        \"east_disp\": displ_sol[0],\n",
    "        \"up_disp\": displ_sol[1],\n",
    "        \"gnss_velocity_e\": gnss_sol[0],\n",
    "        \"gnss_velocity_u\": gnss_sol[1],\n",
    "        \"gnss_velocity_n\": gnss_sol[2],\n",
    "        \"height\": partition[\"height\"].mean(),\n",
    "    })\n",
    "\n",
    "\n",
    "df = df.groupby(\"cid\").apply(\n",
    "    _ls_solve,\n",
    "    meta={\n",
    "        \"east_disp\": \"float64\",\n",
    "        \"up_disp\": \"float64\",\n",
    "        \"gnss_velocity_e\": \"float64\",\n",
    "        \"gnss_velocity_u\": \"float64\",\n",
    "        \"gnss_velocity_n\": \"float64\",\n",
    "        \"height\": \"float64\",\n",
    "    },\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4a6cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_df = df.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2c5707",
   "metadata": {},
   "source": [
    "### Rebuild displacement cubes\n",
    "\n",
    "Once the LS inversion finishes, decode the packed cell IDs back to x/y/time triples and pivot the results into GeoDataFrames so each acquisition date becomes a column aligned with its geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ae16a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(computed_df.index.map(cell_id_to_coords).tolist(), columns=[\"x\", \"y\", \"t\"], index=computed_df.index)\n",
    "data = data.join(computed_df, how=\"outer\")\n",
    "data[\"x\"] += anchor[0]\n",
    "data[\"y\"] += anchor[1]\n",
    "data[\"t_str\"] = (data[\"t\"] + timedelta(days=anchor[2].item())).dt.strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16250dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_component_gdf(data: pd.DataFrame, component: str) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Pivot displacement component by acquisition date and attach geometries.\"\"\"\n",
    "    pivot = (\n",
    "        data.pivot_table(\n",
    "            index=[\"x\", \"y\"],\n",
    "            columns=\"t_str\",\n",
    "            values=component,\n",
    "            aggfunc=\"mean\",\n",
    "        )\n",
    "        .rename_axis(None, axis=1)\n",
    "        .reset_index()\n",
    "    )\n",
    "    static_fields = (\n",
    "        data.sort_values(\"t\")\n",
    "        .drop_duplicates(subset=[\"x\", \"y\"], keep=\"first\")\n",
    "        [[\n",
    "            \"x\",\n",
    "            \"y\",\n",
    "            \"gnss_velocity_e\",\n",
    "            \"gnss_velocity_u\",\n",
    "            \"gnss_velocity_n\",\n",
    "            \"height\",\n",
    "        ]]\n",
    "    )\n",
    "    pivot = pivot.merge(static_fields, on=[\"x\", \"y\"], how=\"left\")\n",
    "    geometry = gpd.points_from_xy(pivot.pop(\"x\"), pivot.pop(\"y\"))\n",
    "    return gpd.GeoDataFrame(pivot, geometry=geometry, crs=\"EPSG:3035\")\n",
    "\n",
    "\n",
    "east_gdf = build_component_gdf(data, \"east_disp\")\n",
    "up_gdf = build_component_gdf(data, \"up_disp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f80512",
   "metadata": {},
   "source": [
    "### Output formatting\n",
    "\n",
    "Apply `model_estimation` to translate LOS-derived stacks into east/up displacement summaries, then persist each component to ZIP and TIFF as required by the EGMS product specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2164f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_format = OutputNamingConvention(\n",
    "    field_list=[\n",
    "        FieldSpec(\"pid\"),\n",
    "        FieldSpec(\"easting\", precision=0),\n",
    "        FieldSpec(\"northing\", precision=0),\n",
    "        FieldSpec(\"height_ortho\", previous_name=\"height\", precision=1),\n",
    "        FieldSpec(\"rmse_ts\", previous_name=\"rmse\", precision=1),\n",
    "        FieldSpec(\"mean_velocity\", precision=1),\n",
    "        FieldSpec(\"mean_velocity_std\", precision=1),\n",
    "        FieldSpec(\"acceleration\", precision=2),\n",
    "        FieldSpec(\"acceleration_std\", precision=2),\n",
    "        FieldSpec(\"seasonality\", precision=1),\n",
    "        FieldSpec(\"seasonality_std\", precision=1),\n",
    "        FieldSpec(\"gnss_velocity_n\", precision=1),\n",
    "        FieldSpec(\"gnss_velocity_e\", precision=1),\n",
    "        FieldSpec(\"gnss_velocity_u\", precision=1),\n",
    "    ],\n",
    "    date_format=\"%Y%m%d\",\n",
    "    date_precision=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5dd3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "wave_length_mm = 55.465760  # Wavelength of Sentinel-1 satellites\n",
    "for final_df, suffix in [(east_gdf, \"east\"), (up_gdf, \"up\")]:\n",
    "    final_df = final_df.clip(aoi)\n",
    "    final_df = model_estimation(final_df, wave_length_mm)\n",
    "    final_df[\"pid\"] = compute_pid(final_df)\n",
    "    final_df = format_dataframe(final_df, output_format)\n",
    "    output_name = generate_output_name(final_df, suffix, aoi_name, 1)\n",
    "    ortho_writer(final_df, output_dir / f\"{output_name}.zip\")\n",
    "    # Produce also a Parquet file for further analysis\n",
    "    final_df.to_parquet(output_dir / f\"{output_name}.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
